{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyhdf\n",
    "import requests\n",
    "from pyhdf.SD import SD\n",
    "from pyhdf.error import HDF4Error\n",
    "from tqdm.auto import tqdm\n",
    "from calendar import monthrange\n",
    "from os import listdir\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2003\n",
    "root_folder = 'C:\\\\data\\\\AIRS\\\\L1B\\\\'\n",
    "\n",
    "sourceurl = \"https://airsl1.gesdisc.eosdis.nasa.gov/data/Aqua_AIRS_Level1/AIRIBRAD.005/\"\n",
    "yearurl = sourceurl + str(year) + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SessionWithHeaderRedirection(requests.Session):\n",
    "    \"\"\"Overrides requests.Session.rebuild_auth to maintain headers when redirected\"\"\"\n",
    "\n",
    "    AUTH_HOST = 'urs.earthdata.nasa.gov'\n",
    "\n",
    "    def __init__(self, username, password):\n",
    "        super().__init__()\n",
    "        self.auth = (username, password)\n",
    "\n",
    "    def rebuild_auth(self, prepared_request, response):\n",
    "        headers = prepared_request.headers\n",
    "        url = prepared_request.url\n",
    "\n",
    "        if 'Authorization' in headers:\n",
    "            original = requests.utils.urlparse(response.request.url).hostname\n",
    "            redirect = requests.utils.urlparse(url).hostname\n",
    "\n",
    "            if original != redirect and redirect != self.AUTH_HOST and original != self.AUTH_HOST:\n",
    "                del headers['Authorization']\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bookends(year):\n",
    "    # receives a year\n",
    "    # returns a 12-item list of tuples, each is the # day start and month's number of days+1\n",
    "    j = 0\n",
    "    k = 1\n",
    "    mon_ss = []\n",
    "    for n in np.arange(1, 13, 1):\n",
    "        j += k\n",
    "        k = monthrange(year, n)[1]\n",
    "        mon_ss.append((j, j+k))\n",
    "    return mon_ss\n",
    "\n",
    "def get_dl_pages(start_day, stop_day, yearurl):\n",
    "    dailypages = []\n",
    "    for n in np.arange(start_day, stop_day, 1):\n",
    "        if n < 10:\n",
    "            dailypages.append(yearurl + \"00\" + str(n) + \"/\")\n",
    "        elif n < 100:\n",
    "            dailypages.append(yearurl + \"0\" + str(n) + \"/\")\n",
    "        else:    \n",
    "            dailypages.append(yearurl + str(n) + \"/\")\n",
    "    return dailypages\n",
    "\n",
    "def get_files(path):\n",
    "    filenames = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "    return filenames\n",
    "\n",
    "def get_links(baseurl):\n",
    "    #baseurl = \"https://airsl1.gesdisc.eosdis.nasa.gov/data/Aqua_AIRS_Level1/AIRIBRAD.005/2003/002/\"\n",
    "    req = Request(baseurl)\n",
    "    html_page = urlopen(req)\n",
    "\n",
    "    soup = BeautifulSoup(html_page)#, \"lxml\")\n",
    "\n",
    "    links = []\n",
    "    for link in soup.findAll('a'):\n",
    "        if 'L1B' in str(link):\n",
    "            if 'xml' not in str(link):\n",
    "                if 'jpg' not in str(link):\n",
    "                    if 'map' not in str(link):\n",
    "                        links.append(baseurl+link.get('href'))\n",
    "    links = sorted([*{*links}])\n",
    "    return links\n",
    "\n",
    "def perform_download(url, output_dir, username, password):\n",
    "    with SessionWithHeaderRedirection(username, password) as http_client:\n",
    "        try:\n",
    "            response = http_client.get(url, stream=True, timeout=10)\n",
    "            response.raise_for_status()  # raise an exception in case of http errors\n",
    "\n",
    "            filename = os.path.join(output_dir, url.split('/')[-1])\n",
    "            with open(filename, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024 * 1024):\n",
    "                    f.write(chunk)\n",
    "\n",
    "        except requests.exceptions.HTTPError:\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            raise Exception('Unhandled exception: %s' % e)\n",
    "\n",
    "def download_files(urls, count_callback=None):\n",
    "    \"\"\"Downloads all necessary files that are not yet stored on the disk using multiple processes.\n",
    "    download_finished_callback and error_callback cannot be bound or unbound methods in Windows, so pass\n",
    "    functions instead.\n",
    "    \"\"\"\n",
    "    #urls = list(self.filter_files(urls, self._storage_directory))\n",
    "\n",
    "    if count_callback is not None:\n",
    "        count_callback(len(urls))\n",
    "\n",
    "    #process_args = [(url, self._storage_directory, self._username, self._password) for url in urls]\n",
    "    process_args = [(url, 'C:\\\\data\\\\AIRS\\\\L1B\\\\', 'USERNAME', 'PASSWORD') for url in urls]\n",
    "\n",
    "    with Pool(processes=5) as pool:\n",
    "        pool.starmap_async(perform_download, process_args, callback=lambda x: None, error_callback=lambda x: None)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    # check if any files failed to download, and return false if so\n",
    "    #urls = list(self.filter_files(urls, self._storage_directory))\n",
    "    #return len(urls) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mon_ss = bookends(year)  # month_ss = 'Month Start & Stop'\n",
    "\n",
    "for month in tqdm(np.arange(1, 13, 1), \"Month\", ncols=400, position=0):\n",
    "    #if month > 5:\n",
    "    #    continue\n",
    "    if month > 9:\n",
    "        n = str(month)+'\\\\'\n",
    "    else:\n",
    "        n = '0'+str(month)+'\\\\'\n",
    "    save_path = root_folder+n\n",
    "    \n",
    "    # Get a list of the files in that month's folder:\n",
    "    existing_files = get_files(save_path)\n",
    "    \n",
    "    # Make dailypages list for the current month\n",
    "    dailypages = get_dl_pages(mon_ss[month-1][0], mon_ss[month-1][1], yearurl)\n",
    "    \n",
    "    for page in tqdm(dailypages, \"Day\", ncols=400, position=1):  # One website per day\n",
    "        try:\n",
    "            links = get_links(page)  # Grab all links from the one website (expect <=240 links)\n",
    "        except:\n",
    "            print(\"Can't access page\", page, \"please check if it exists.\")\n",
    "            break\n",
    "        download_files(links)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
