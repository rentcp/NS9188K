{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netrc import netrc\n",
    "from subprocess import Popen\n",
    "from getpass import getpass\n",
    "import platform\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up credential files (only run the first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code only needs to be run once, then can be skipped in future instances:\n",
    "# This code comes from: https://disc.gsfc.nasa.gov/data-access#python-requests\n",
    "\n",
    "if 1 == 1:\n",
    "    urs = 'urs.earthdata.nasa.gov'    # Earthdata URL to call for authentication\n",
    "    prompts = ['Enter NASA Earthdata Login Username \\n(or create an account at urs.earthdata.nasa.gov): ',\n",
    "               'Enter NASA Earthdata Login Password: ']\n",
    "\n",
    "    homeDir = os.path.expanduser(\"~\") + os.sep\n",
    "\n",
    "    with open(homeDir + '.netrc', 'w') as file:\n",
    "        file.write('machine {} login {} password {}'.format(urs, getpass(prompt=prompts[0]), getpass(prompt=prompts[1])))\n",
    "        file.close()\n",
    "    with open(homeDir + '.urs_cookies', 'w') as file:\n",
    "        file.write('')\n",
    "        file.close()\n",
    "    with open(homeDir + '.dodsrc', 'w') as file:\n",
    "        file.write('HTTP.COOKIEJAR={}.urs_cookies\\n'.format(homeDir))\n",
    "        file.write('HTTP.NETRC={}.netrc'.format(homeDir))\n",
    "        file.close()\n",
    "\n",
    "    print('Saved .netrc, .urs_cookies, and .dodsrc to:', homeDir)\n",
    "\n",
    "    # Set appropriate permissions for Linux/macOS\n",
    "    if platform.system() != \"Windows\":\n",
    "        Popen('chmod og-rw ~/.netrc', shell=True)\n",
    "    else:\n",
    "        # Copy dodsrc to working directory in Windows  \n",
    "        shutil.copy2(homeDir + '.dodsrc', os.getcwd())\n",
    "        print('Copied .dodsrc to:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import requests\n",
    "from tqdm.auto import tqdm\n",
    "from calendar import monthrange\n",
    "from os import listdir\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2021\n",
    "root_folder = 'C:\\\\data\\\\AIRS\\\\L1B\\\\'\n",
    "\n",
    "sourceurl = \"https://airsl1.gesdisc.eosdis.nasa.gov/data/Aqua_AIRS_Level1/AIRIBRAD.005/\"\n",
    "yearurl = sourceurl + str(year) + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bookends(year):\n",
    "    # receives a year\n",
    "    # returns a 12-item list of tuples, each is the # day start and month's number of days+1\n",
    "    j = 0\n",
    "    k = 1\n",
    "    mon_ss = []\n",
    "    for n in np.arange(1, 13, 1):\n",
    "        j += k\n",
    "        k = monthrange(year, n)[1]\n",
    "        mon_ss.append((j, j+k))\n",
    "    return mon_ss\n",
    "\n",
    "def get_dl_pages(start_day, stop_day, yearurl):\n",
    "    dailypages = []\n",
    "    for n in np.arange(start_day, stop_day, 1):\n",
    "        if n < 10:\n",
    "            dailypages.append(yearurl + \"00\" + str(n) + \"/\")\n",
    "        elif n < 100:\n",
    "            dailypages.append(yearurl + \"0\" + str(n) + \"/\")\n",
    "        else:    \n",
    "            dailypages.append(yearurl + str(n) + \"/\")\n",
    "    return dailypages\n",
    "\n",
    "def get_links(baseurl):\n",
    "    req = Request(baseurl)\n",
    "    html_page = urlopen(req)\n",
    "\n",
    "    soup = BeautifulSoup(html_page)#, \"lxml\")\n",
    "\n",
    "    links = []\n",
    "    for link in soup.findAll('a'):\n",
    "        if 'L1B' in str(link):\n",
    "            if 'xml' not in str(link):\n",
    "                if 'jpg' not in str(link):\n",
    "                    if 'map' not in str(link):\n",
    "                        links.append(baseurl+link.get('href'))\n",
    "    links = sorted([*{*links}])\n",
    "    return links\n",
    "\n",
    "def perform_download(URL, FOLDER):\n",
    "    result = requests.get(URL)\n",
    "    FILENAME = FOLDER + URL.rsplit('/')[-1]\n",
    "    try:\n",
    "        result.raise_for_status()\n",
    "        f = open(FILENAME,'wb')\n",
    "        f.write(result.content)\n",
    "        f.close()\n",
    "        #print('contents of URL written to '+FILENAME)\n",
    "    except:\n",
    "        print('requests.get() returned an error code '+str(result.status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mon_ss = bookends(year)  # month_ss = 'Month Start & Stop'\n",
    "bad_attempts = []\n",
    "for month in tqdm(np.arange(1, 13, 1), \"Month\", ncols=400, position=0):\n",
    "    if month > 9:\n",
    "        n = str(month)+'\\\\'\n",
    "    else:\n",
    "        n = '0'+str(month)+'\\\\'\n",
    "    save_path = root_folder+str(year) + '\\\\' + n\n",
    "    \n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "            \n",
    "    # Make dailypages list for the current month\n",
    "    dailypages = get_dl_pages(mon_ss[month-1][0], mon_ss[month-1][1], yearurl)\n",
    "    \n",
    "    for page in tqdm(dailypages, \"Day\", ncols=400, position=1, leave=False):  # One website per day\n",
    "        try:\n",
    "            links = get_links(page)  # Grab all links from the one website (expect <=240 links)\n",
    "        except:\n",
    "            print(\"Can't access page\", page, \"please check if it exists.\")\n",
    "            break\n",
    "        \n",
    "        existing_files = glob.glob(save_path + '\\\\*.hdf')\n",
    "        existing_files = [file.rsplit('\\\\')[-1] for file in existing_files]\n",
    "        \n",
    "        for link in links:\n",
    "            if not link.rsplit('/')[-1] in existing_files:\n",
    "                try:\n",
    "                    perform_download(link, save_path)\n",
    "                except:\n",
    "                    print(\"Bad download attempt logged.\")\n",
    "                    bad_attempts.append((link, save_path))\n",
    "\n",
    "print(year, 'complete. There are', len(bad_attempts), \"files unsuccessfully attempted. Retrying...\")\n",
    "for link, save_path in bad_attempts:\n",
    "    perform_download(link, save_path)\n",
    "\n",
    "print('Retries complete. Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
